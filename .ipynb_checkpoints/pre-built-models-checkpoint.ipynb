{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1fbb6c3",
   "metadata": {},
   "source": [
    "# Model 2 & 3 - Pre-built Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cf9421",
   "metadata": {},
   "source": [
    "The following two models I will train will come from the SciKit Learn open source library. The first will be a Naive Bayes classifier similar to the one I previously built from scratch. The second will be a Linear SVC which is the estimator recommended by the library's documentation for this type of problem (classification with less than 100k labeled data samples). I expect these models to be more efficient and accurate than the one built from scratch as they were optimized by experts in the community. The next step will be to evaluate and compare all the models on the unseen testing dataset.\n",
    "\n",
    "![SciKit Learn Estimator Decision Tree](img/EDA/estimator_decision_tree.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e2c048a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c419ad5",
   "metadata": {},
   "source": [
    "## Import Training and Validation Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6faec84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the path for the processed datasets\n",
    "PATH = \"data/processed/\"\n",
    "\n",
    "# read the training dataset\n",
    "hp_sentences_train = pd.read_csv(f\"{PATH}training_df.csv\")\n",
    "\n",
    "# read the validation dataset\n",
    "hp_sentences_val = pd.read_csv(f\"{PATH}validation_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8ca9dda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>book</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A wild-looking old woman dressed all in green ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Harry was thinking about this time yesterday a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>He had been down at Hagrid’s hut, helping him ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>“We’re looking for a big, old-fashioned one — ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I forbid you to tell the boy anything!” A brav...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  book\n",
       "0  A wild-looking old woman dressed all in green ...     1\n",
       "1  Harry was thinking about this time yesterday a...     1\n",
       "2  He had been down at Hagrid’s hut, helping him ...     1\n",
       "3  “We’re looking for a big, old-fashioned one — ...     1\n",
       "4  I forbid you to tell the boy anything!” A brav...     1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the first 5 rows of the training dataset\n",
    "hp_sentences_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abdc7fdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>book</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>“She obviously makes more of an effort if you’...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We’ve eaten all our food and you still seem to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Please cheer up, Hagrid, we saved the Stone, i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>He gave his father a sharp tap on the head wit...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>He kept threatening to tell her what really bi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  book\n",
       "0  “She obviously makes more of an effort if you’...     1\n",
       "1  We’ve eaten all our food and you still seem to...     1\n",
       "2  Please cheer up, Hagrid, we saved the Stone, i...     1\n",
       "3  He gave his father a sharp tap on the head wit...     1\n",
       "4  He kept threatening to tell her what really bi...     1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the first 5 rows of the validation dataset\n",
    "hp_sentences_val.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377a5631",
   "metadata": {},
   "source": [
    "## Naive Bayes Classification Model\n",
    "The first pre-built model I used is a Naive Bayes text classifier similar to the one I built from scratch. Recall, my model achieved an accuracy of 63.6% on the training data and 37.2% on the validation data. This model is expected to be slightly more accurate but considerably more efficient. It also has the advantage of significantly reducing the development time through the use of pre-built models and functions.\n",
    "\n",
    "Similar to the custom model, the first step is to preprocess the data. The same tasks previously performed manually are handled by the CountVectorizer model which preprocesses, tokenizes and filters stopwords from the raw sentences. It then creates a vector where each row represents a sentence and each column represents the number of times that word appeared in the sentence. \n",
    "\n",
    "The second step is to use the TfidfTransformer model which divides the number of occurences of each word in a sentence by the total number of words in the sentenece to avoid discrependancies between shorter and longer sentences. It then downscales the weights for the words that occur often across sentences as they are less informative than words that appear in few sentences.\n",
    "\n",
    "The final step is to pass the generated word vector in the naive bayes multinomial classifier to either train the model or generate a prediction.\n",
    "\n",
    "These three steps are consolidated into one pipeline that can be used to train the model and also make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afd8987f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pipeline with the three steps required to train the classifier and make predictions\n",
    "hp_classifier_nb = Pipeline([\n",
    "    ('count_vect', CountVectorizer()), # create a word count vector\n",
    "    ('freq_vect', TfidfTransformer()), # normalize the term frequencies\n",
    "    ('classify', MultinomialNB()) # use a Naive Bayes multinomial classifier\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfba2cfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('count_vect', CountVectorizer()),\n",
       "                ('freq_vect', TfidfTransformer()),\n",
       "                ('classify', MultinomialNB())])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model on the sentences in the training dataset\n",
    "hp_classifier_nb.fit(hp_sentences_train[\"sentence\"], hp_sentences_train[\"book\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f15d7b7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.496907321030217"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict the books for the sentences in the training dataset\n",
    "book_predictions_train_nb = hp_classifier_nb.predict(hp_sentences_train[\"sentence\"])\n",
    "\n",
    "# test the classifier's accuracy on the training data\n",
    "np.mean(book_predictions_train_nb == hp_sentences_train[\"book\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6c66087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38787825006655424"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict the books for the sentences in the validation dataset\n",
    "book_predictions_val_nb = hp_classifier_nb.predict(hp_sentences_val[\"sentence\"])\n",
    "\n",
    "# test the classifier's accuracy on the training data\n",
    "np.mean(book_predictions_val_nb == hp_sentences_val[\"book\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c5dc66",
   "metadata": {},
   "source": [
    "As will almost always be the case, the model is less accurate on the validation dataset compared to the training data because some words in the validation dataset don't appear in the training data or have different meanings.\n",
    "\n",
    "The accuracy metric for the model is 38.8% which is slightly superior than the accuracy achieved by the custom model (37.2%). However it was significantly faster to train and predict. It is also worth noting that the custom model was more accurate on the training data compared to the pre-built version."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6522243",
   "metadata": {},
   "source": [
    "## Linear SVC Classification Model\n",
    "The second pre-built model I used is a Linear SVC (Support Vector Classifier) as was recommended by the ScikitLearn document as described above. The steps used to train the model and make predictions are the same as for the Naive Bayes model explained above with the only difference being the pre-built model used (Linear SVC instead of Naive Bayes).\n",
    "\n",
    "This is another benefit of using pre-built models from a library. I am able in very little time to compare two (and more) models to see which one is most accurate and efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7e57e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pipeline with the three steps required to train the classifier and make predictions\n",
    "hp_classifier_svc = Pipeline([\n",
    "    ('count_vect', CountVectorizer()), # create a word count vector\n",
    "    ('freq_vect', TfidfTransformer()), # normalize the term frequencies\n",
    "    ('classify', LinearSVC()) # use a Linear SVC classifier\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "430e2413",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('count_vect', CountVectorizer()),\n",
       "                ('freq_vect', TfidfTransformer()), ('classify', LinearSVC())])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model on the sentences in the training dataset\n",
    "hp_classifier_svc.fit(hp_sentences_train[\"sentence\"], hp_sentences_train[\"book\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "764cb6a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7676688298519571"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict the books for the sentences in the training dataset\n",
    "book_predictions_train_svc = hp_classifier_svc.predict(hp_sentences_train[\"sentence\"])\n",
    "\n",
    "# test the classifier's accuracy on the training data\n",
    "np.mean(book_predictions_train_svc == hp_sentences_train[\"book\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7a8652b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.453367645753838"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict the books for the sentences in the validation dataset\n",
    "book_predictions_val_svc = hp_classifier_svc.predict(hp_sentences_val[\"sentence\"])\n",
    "\n",
    "# test the classifier's accuracy on the training data\n",
    "np.mean(book_predictions_val_svc == hp_sentences_val[\"book\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f75c49",
   "metadata": {},
   "source": [
    "With an accuracy metric of 45.33% on the validation dataset, it seems that the Linear SVC model is the most accurate of the three options. While this accuracy is still relatively low, it is a significant improvement from our baseline and two previous models. \n",
    "\n",
    "The following notebook will seek to confirm that this is the most accurate model and attempt to improve it's accuracy through hyperparameter tuning and error analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4227c16e",
   "metadata": {},
   "source": [
    "# Notebook Complete!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
